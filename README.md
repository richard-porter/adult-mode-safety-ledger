# Adult Mode Safety Ledger

**Public safety scorecard for high-gain AI conversational features.**

Binary architectural tests. Pre-launch criteria. Five AI models evaluated against the same standards.

-----

## What This Is

This is a public accountability tool. It asks a simple question: before an AI platform launches features that involve high-gain conversational contexts — adult content, emotional intimacy, therapeutic interaction — has the architecture been tested for the failure modes that cause documented harm?

The tests are binary. Pass or fail. The criteria are published. The results are public.

-----

## Contents

|File                                 |Description                                 |
|-------------------------------------|--------------------------------------------|
|`methodology.docx`                   |How the tests work and what they measure    |
|`v1_0-proactive-safety-criteria.docx`|The full criteria set (v1.0)                |
|`2026-02-11-initial-baseline.docx`   |Baseline assessment date and conditions     |
|`chatgpt-openai.docx`                |ChatGPT / OpenAI scorecard                  |
|`claude-anthropic.docx`              |Claude / Anthropic scorecard                |
|`deepseek.docx`                      |DeepSeek scorecard                          |
|`gemini-google.docx`                 |Gemini / Google scorecard                   |
|`grok-xai.docx`                      |Grok / xAI scorecard                        |
|`open-invitation-to-models.docx`     |Open invitation for AI models to self-assess|
|`CHANGELOG.docx`                     |Version history                             |

-----

## Related Work

- [The Frozen Kernel](https://github.com/richard-porter/frozen-kernel) — The governance architecture these tests are derived from
- [AI Collaboration Field Guide](https://github.com/richard-porter/ai-collaboration-field-guide) — Practical skills for humans working with AI
- [Therapy Mode Safety Checklist](https://github.com/richard-porter/therapy-mode-safety-checklist.md) — Six-question safety gate for AI systems interacting with vulnerable humans
- [Dimensional Authorship](https://github.com/richard-porter/dimensional-authorship) — The research home documenting the full case study

-----

## License

Released for public benefit. Attribution appreciated but not required. If you build on this work, the only ask: keep humans sovereign.
# Suggested GitHub Topics

`ai-safety` · `ai-psychosis` · `ai-governance` · `llm-safety` · `sycophancy` · `ai-alignment` · `behavioral-safety` · `deterministic-safety` · `human-ai-interaction` · `ai-ethics` · `mental-health` · `ai-accountability` · `guardrails` · `responsible-ai`
